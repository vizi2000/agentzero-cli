# Agent Zero CLI Configuration
# Copy to .env and fill in your values

# ============================================
# BACKEND PRIORITY (uses first available):
# 1. Local LLM (SAFEST - data stays on your network)
# 2. Agent Zero API (self-hosted)
# 3. OpenRouter (cloud LLMs)
# 4. Deterministic (no LLM, pattern matching)
# ============================================

# --- Option 1: Local LLM (SAFEST - RECOMMENDED) ---
# Your prompts NEVER leave your network!
# Works with: LM Studio, Ollama, LocalAI, Text Generation WebUI
#
# LM Studio (default port 1234):
# LOCAL_LLM_URL=http://localhost:1234/v1
#
# Ollama (default port 11434):
# LOCAL_LLM_URL=http://localhost:11434/v1
#
# Remote LM Studio on another machine:
# LOCAL_LLM_URL=http://192.168.1.100:1234/v1
#
# Optional: specify model name (auto-detected if not set)
# LOCAL_LLM_MODEL=mistralai/ministral-3-3b

# --- Option 2: Self-hosted Agent Zero ---
# URL to your Agent Zero instance
# AGENTZERO_API_URL=http://localhost:50001/api_message
# AGENTZERO_API_KEY=your-api-key-if-required

# --- Option 3: OpenRouter (cloud - data leaves your network) ---
# Get API key at: https://openrouter.ai/keys
# OPENROUTER_API_KEY=sk-or-v1-xxx
#
# Optional: specify models for load balancing (comma-separated)
# OPENROUTER_MODELS=mistralai/devstral-2512:free,qwen/qwen3-coder:free

# --- Option 4: Deterministic mode ---
# No configuration needed - works offline with pattern matching
# Limited functionality but no API required

# ============================================
# SECURITY SETTINGS
# ============================================

# Security mode: paranoid | balanced | god_mode
AGENT_SECURITY_MODE=balanced

# ============================================
# DEBUG
# ============================================

# Enable debug logging
AGENT_DEBUG=false
